{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXYk_ipcx4-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c089a722-6ea8-4c63-ef11-3ddce0225f18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Corpus/Adobe.txt',\n",
              " '/content/Corpus/Amazon.txt',\n",
              " '/content/Corpus/apple.txt',\n",
              " '/content/Corpus/Binance.txt',\n",
              " '/content/Corpus/bing.txt',\n",
              " '/content/Corpus/blackberry.txt',\n",
              " '/content/Corpus/canva.txt',\n",
              " '/content/Corpus/Dell.txt',\n",
              " '/content/Corpus/Discord.txt',\n",
              " '/content/Corpus/flipkart.txt',\n",
              " '/content/Corpus/google.txt',\n",
              " '/content/Corpus/HP.txt',\n",
              " '/content/Corpus/huawei.txt',\n",
              " '/content/Corpus/instagram.txt',\n",
              " '/content/Corpus/Lenovo.txt',\n",
              " '/content/Corpus/levis.txt',\n",
              " '/content/Corpus/messenger.txt',\n",
              " '/content/Corpus/microsoft.txt',\n",
              " '/content/Corpus/motorola.txt',\n",
              " '/content/Corpus/nike.txt',\n",
              " '/content/Corpus/nokia.txt',\n",
              " '/content/Corpus/Ola.txt',\n",
              " '/content/Corpus/operating.txt',\n",
              " '/content/Corpus/paypal.txt',\n",
              " '/content/Corpus/puma.txt',\n",
              " '/content/Corpus/reddit.txt',\n",
              " '/content/Corpus/reliance.txt',\n",
              " '/content/Corpus/samsung.txt',\n",
              " '/content/Corpus/shakespeare.txt',\n",
              " '/content/Corpus/skype.txt',\n",
              " '/content/Corpus/sony.txt',\n",
              " '/content/Corpus/spotify.txt',\n",
              " '/content/Corpus/steam.txt',\n",
              " '/content/Corpus/swiggy.txt',\n",
              " '/content/Corpus/telegram.txt',\n",
              " '/content/Corpus/Uber.txt',\n",
              " '/content/Corpus/volkswagen.txt',\n",
              " '/content/Corpus/whatsapp.txt',\n",
              " '/content/Corpus/yahoo.txt',\n",
              " '/content/Corpus/youtube.txt',\n",
              " '/content/Corpus/zomato.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/drive/folders/1D3sM984cM_aKdgr6HfxM7UvT_n526A3e?usp=share_link'\n",
        "opath = '/content'\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "import numpy as numpy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "w-DraMVjzafQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_preprocessing(text):\n",
        "  # Removes punctuations.\n",
        "  p_text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # Removes newline characters.\n",
        "  p_text = p_text.translate(str.maketrans({'\\n': ' '}))\n",
        "  \n",
        "  # Removes tab characters.\n",
        "  p_text = p_text.translate(str.maketrans({'\\t': ''}))\n",
        "\n",
        "  # lowers the text.\n",
        "  p_text = p_text.lower()\n",
        "\n",
        "  # Removes whitespaces at the beginning and end of the text.\n",
        "  p_text = p_text.strip()\n",
        "\n",
        "  # Substitutes multiple whitespaces with a single whitespace.\n",
        "  p_text = re.sub(r\"\\s\\s+\", \" \", p_text) \n",
        "\n",
        "  # Splitting text into tokens\n",
        "  p_tokens = re.split('\\W+', p_text)\n",
        "\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "  # Stopword and single character Removal\n",
        "  p_tokens = [token for token in p_tokens if token not in stopwords and len(token) > 1] \n",
        "\n",
        "  # Stem Tokens\n",
        "  stemmer = PorterStemmer()\n",
        "  p_tokens = [stemmer.stem(token) for token in p_tokens]\n",
        "\n",
        "  # Lemmatize tokens\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  p_tokens = [lemmatizer.lemmatizer(token) for token in p_tokens]\n",
        "\n",
        "  return p_tokens"
      ],
      "metadata": {
        "id": "MMb9-C_V_QFn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_doc_dict():\n",
        "  doc_dict = {}\n",
        "  file_list = glob.glob('/content/corpus/*')\n",
        "  for file in file_list:\n",
        "    file_name = file.split('/')[-1]\n",
        "    with open(file, 'r') as file:\n",
        "      data = file.read()\n",
        "      doc_dict[file_name] = document_preprocessing(data)\n",
        "  return doc_dict"
      ],
      "metadata": {
        "id": "PWKOmDoIGZuI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_dict = get_doc_dict()\n",
        "# for doc_name, data in doc_dic.items():\n",
        "# print(doc_name, '\\n', data)"
      ],
      "metadata": {
        "id": "HJFNYvcVJiOP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordList(doc_dict):\n",
        "  wordList = []\n",
        "  for doc in doc_dict:\n",
        "    wordList += doc_dict[doc]\n",
        "    return wordList"
      ],
      "metadata": {
        "id": "-VMYktAoKMS_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_doc(vocabulary, doc_dict):\n",
        "  tf_docs = {}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    tf_docs[doc_id] = {}\n",
        "\n",
        "  for word in vocabulary:\n",
        "    for doc_id, doc in doc_dict.items():\n",
        "      tf_docs[doc_id][word] = doc.count(word)\n",
        "      return tf_docs"
      ],
      "metadata": {
        "id": "68AFDIhXK1Fe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_doc(vocabulary, doc_dict):\n",
        "  df = {}\n",
        "  for word in vocabulary:\n",
        "    freq = 0\n",
        "    for doc in doc_dict.values():\n",
        "      if word in word_tokenize(doc.lower().strip()):\n",
        "        freq += 1\n",
        "        df[word] = freq\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "vazCzxu0MHvk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def idf_doc(vocabulary, doc_freq, length):\n",
        "  idf = {}\n",
        "  for word in vocabulary:\n",
        "    idf[word] = np.log10((length + 1) / doc_freq[word])\n",
        "    return idf"
      ],
      "metadata": {
        "id": "6p6UgFS_NVup"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}